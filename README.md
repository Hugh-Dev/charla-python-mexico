# *T√≠tulo: Python y los Esquemas ETL: El Pivote Esencial para la Inteligencia Artificial*

## *¬© Python M√©xico 2024*

[Conceptos b√°sicos](batch-release-etl/docs/index.md)


## **Requerimientos para la demo**

[![pypi](https://pypi.org/static/images/logo-small.8998e9d1.svg)](https://pypi.org)
| | |
| --- | --- |
| Pandas | [![Pandas Latest Release](https://img.shields.io/pypi/v/pandas.svg)](https://pypi.org/project/pandas/) [![PyPI Downloads](https://img.shields.io/pypi/dm/pandas.svg?label=PyPI%20downloads)](https://pypi.org/project/pandas/) [![Conda Latest Release](https://anaconda.org/conda-forge/pandas/badges/version.svg)](https://anaconda.org/conda-forge/pandas) [![Conda Downloads](https://img.shields.io/conda/dn/conda-forge/pandas.svg?label=Conda%20downloads)](https://anaconda.org/conda-forge/pandas) |
| numpy | [![Numpy Latest Release](https://img.shields.io/pypi/v/numpy.svg)](https://pypi.org/project/numpy/) [![PyPI Downloads](https://img.shields.io/pypi/dm/numpy.svg?label=PyPI%20downloads)](https://pypi.org/project/numpy/) [![Conda Latest Release](https://anaconda.org/conda-forge/numpy/badges/version.svg)](https://anaconda.org/conda-forge/numpy) [![Conda Downloads](https://img.shields.io/conda/dn/conda-forge/numpy.svg?label=Conda%20downloads)](https://anaconda.org/conda-forge/numpy) |
| Requests | [![Requests Latest Release](https://img.shields.io/pypi/v/requests.svg)](https://pypi.org/project/requests/) [![PyPI Downloads](https://img.shields.io/pypi/dm/requests.svg?label=PyPI%20downloads)](https://pypi.org/project/requests/) [![Conda Latest Release](https://anaconda.org/conda-forge/requests/badges/version.svg)](https://anaconda.org/conda-forge/requests) [![Conda Downloads](https://img.shields.io/conda/dn/conda-forge/requests.svg?label=Conda%20downloads)](https://anaconda.org/conda-forge/requests) |

## **Descripci√≥n**

Quiza todos hemos concentrado la mirada en la Inteligencia Artificial (IA), preocupados por temas como Machine Learning, Deep Learning o IA generative pero estos modelos dependen en gran medida de datos de alta calidad y grandes vol√∫menes de datos. Los datos pueden provenir de diversas fuentes e aqui donde entran en juego los esquemas ETL.

S√≠, es entonces un proceso ETL (Extracci√≥n, Transformaci√≥n y Carga) vital para la Inteligencia Artificial (IA) cuales son las razones. A continuaci√≥n, te explico c√≥mo y por qu√©:

### **Extracci√≥n (E)**

- **Obtenci√≥n de Datos**: Los modelos de IA y Machine Learning (ML) dependen en gran medida de datos de alta calidad y grandes vol√∫menes de datos. Los datos pueden provenir de diversas fuentes como bases de datos, archivos, APIs, sensores IoT, redes sociales, etc. La fase de extracci√≥n se encarga de recoger estos datos de las m√∫ltiples fuentes.
- **Variedad de Datos**: En IA, es com√∫n trabajar con datos estructurados (bases de datos relacionales), semi-estructurados (XML, JSON) y no estructurados (texto, im√°genes, audio). La extracci√≥n debe ser capaz de manejar esta variedad de datos.

### **Transformaci√≥n (T)**

- **Limpieza de Datos**: Los datos crudos a menudo contienen errores, duplicados o valores faltantes. La transformaci√≥n incluye la limpieza de datos, lo que mejora la calidad de los datos y, en consecuencia, la precisi√≥n de los modelos de IA.
- **Normalizaci√≥n y Formateo**: Los datos provenientes de diferentes fuentes pueden tener diferentes formatos y escalas. La transformaci√≥n normaliza estos datos para asegurar que sean consistentes y adecuados para el an√°lisis.
- **Enriquecimiento de Datos**: A veces, los datos extra√≠dos necesitan ser enriquecidos con informaci√≥n adicional, como datos geogr√°ficos o demogr√°ficos, para proporcionar contexto adicional √∫til para el modelo de IA.
- **Feature Engineering**: En IA, es crucial transformar los datos crudos en caracter√≠sticas que los modelos puedan entender y usar efectivamente. Este proceso incluye t√©cnicas como la creaci√≥n de nuevas variables a partir de las existentes, transformaci√≥n de variables categ√≥ricas a num√©ricas, etc.

### **Carga (L)**

- **Almacenamiento Eficiente**: Los datos transformados deben ser almacenados en un sistema accesible y eficiente, como un data warehouse, data lake, o bases de datos NoSQL. Esto permite que los datos est√©n disponibles para el entrenamiento y la inferencia de los modelos de IA.
- **Escalabilidad**: Los sistemas de almacenamiento deben ser escalables para manejar grandes vol√∫menes de datos y permitir consultas r√°pidas y eficientes.

### **Importancia para la IA**

- **Calidad de Datos**: Los modelos de IA son tan buenos como la calidad de los datos en los que se entrenan. Un proceso ETL bien dise√±ado asegura que los datos sean precisos, limpios y relevantes.
- **Preprocesamiento**: El ETL se encarga de la mayor parte del preprocesamiento de datos, una etapa cr√≠tica en cualquier proyecto de IA.
- **Automatizaci√≥n y Eficiencia**: Los procesos ETL pueden ser automatizados, lo que permite un flujo constante y eficiente de datos para entrenar y actualizar los modelos de IA.

El proceso ETL es fundamental para la IA porque asegura que los datos necesarios est√©n disponibles, sean de alta calidad y est√©n en el formato correcto. Sin un proceso ETL robusto, la creaci√≥n, entrenamiento y despliegue de modelos de IA ser√≠a mucho m√°s complicado y menos eficiente. Por lo tanto, ETL es una pieza clave en el pipeline de datos que impulsa la Inteligencia Artificial.

## ¬øCuales son los lenguajes de programacion usados para el desarrollo de ETL?

Para el desarrollo de procesos ETL (Extracci√≥n, Transformaci√≥n y Carga), se utilizan diversos lenguajes de programaci√≥n y herramientas. Los lenguajes m√°s com√∫nmente utilizados son:

### **Python**

- **Popularidad**: Python es uno de los lenguajes m√°s populares para ETL debido a su simplicidad, versatilidad y la gran cantidad de bibliotecas y frameworks disponibles.
- **Bibliotecas**: Pandas, NumPy, SQLAlchemy, PySpark, Dask.
- **Frameworks**: Apache Airflow, Luigi, Kedro, Bonobo.

### **SQL**

- **Uso**: SQL es fundamental en ETL para la manipulaci√≥n de datos almacenados en bases de datos relacionales. Se utiliza para la extracci√≥n de datos, transformaci√≥n (a trav√©s de consultas y procedimientos almacenados) y carga en sistemas de destino.
- **Variantes**: Transact-SQL (T-SQL), PL/SQL, MySQL, PostgreSQL.

### **Java**

- **Popularidad**: Java es ampliamente utilizado en ETL por su robustez y escalabilidad, especialmente en entornos empresariales.
- **Frameworks y Herramientas**: Apache NiFi, Talend, Apache Hop, Spring Batch.

### **Scala**

- **Uso en Big Data**: Scala es el lenguaje principal para Apache Spark, una herramienta de procesamiento de datos a gran escala.
- **Popularidad**: Utilizado en ambientes donde el procesamiento de grandes vol√∫menes de datos es cr√≠tico.

### **R**

- **An√°lisis y Estad√≠sticas**: R es utilizado principalmente para an√°lisis de datos y transformaci√≥n debido a su potente conjunto de bibliotecas estad√≠sticas.
- **Bibliotecas**: dplyr, data.table, tidyverse.

### **Shell Scripting (Bash, PowerShell)**

- **Automatizaci√≥n**: Se utiliza para la automatizaci√≥n de tareas ETL en sistemas operativos Unix/Linux y Windows.
- **Uso**: Ideal para la integraci√≥n de diferentes herramientas y el movimiento de archivos.

### **C#**

- **Uso Empresarial**: C# se utiliza en entornos empresariales, especialmente con herramientas de Microsoft como SQL Server Integration Services (SSIS).
- **Herramientas**: SSIS, Azure Data Factory.

### **Go**

- **Eficiencia y Rendimiento**: Go es utilizado por su rendimiento y eficiencia, especialmente en la creaci√≥n de pipelines ETL distribuidos.
- **Herramientas**: Pachyderm.

### **Herramientas y Plataformas de ETL**

Adem√°s de los lenguajes de programaci√≥n, existen herramientas espec√≠ficas para ETL que pueden ser configuradas y manejadas sin necesidad de programaci√≥n extensiva:

- **Talend**: Plataforma de integraci√≥n de datos que soporta m√∫ltiples lenguajes y ofrece una interfaz gr√°fica para dise√±ar procesos ETL.
- **Informatica PowerCenter**: Herramienta robusta y ampliamente utilizada en empresas para la integraci√≥n de datos.
- **Microsoft SQL Server Integration Services (SSIS)**: Herramienta de ETL incluida en Microsoft SQL Server.
- **Apache Nifi**: Herramienta para automatizar el flujo de datos entre sistemas.
- **Alteryx**: Plataforma de an√°lisis de datos que facilita la preparaci√≥n y mezcla de datos.

La elecci√≥n del lenguaje de programaci√≥n y la herramienta de ETL depende de varios factores, el entorno de trabajo, los requisitos del proyecto, el volumen de datos, y las habilidades del equipo. **Python** es uno de los m√°s vers√°tiles y ampliamente utilizados, a pesar
de que cada lenguaje y herramienta tiene su nicho y ventajas espec√≠ficas.

## Que empresas reconocidas implementan ETL

Muchas empresas de diversos sectores implementan procesos ETL (Extracci√≥n, Transformaci√≥n y Carga) para gestionar y procesar grandes vol√∫menes de datos. Algunas de las empresas m√°s importantes que utilizan ETL son:

### **Amazon**

- **Amazon Web Services (AWS)**: Utiliza y ofrece servicios ETL como AWS Glue, que facilita la preparaci√≥n y carga de datos para an√°lisis y machine learning.

### **Google**

- **Google Cloud Platform (GCP)**: Ofrece servicios ETL como Google Cloud Dataflow y Google Cloud Dataprep para la integraci√≥n y preparaci√≥n de datos.

### **Microsoft**

- **Azure**: Proporciona herramientas como Azure Data Factory y SQL Server Integration Services (SSIS) para procesos ETL en la nube y on-premises.

### **IBM**

- **IBM DataStage**: Una herramienta ETL robusta que es parte de la plataforma IBM InfoSphere para la integraci√≥n de datos.

### **Oracle**

- **Oracle Data Integrator (ODI)**: Utilizada para la integraci√≥n de datos en entornos empresariales, soportando procesos ETL complejos.

### **Salesforce**

- **MuleSoft**: Parte de Salesforce, proporciona capacidades de integraci√≥n de datos y servicios ETL para conectar aplicaciones, datos y dispositivos.

### **Facebook**

- Utiliza procesos ETL extensivos para gestionar datos masivos y alimentar su plataforma de an√°lisis interno, as√≠ como servicios de terceros como Apache Hadoop y Apache Hive.

### **Netflix**

- Implementa ETL con herramientas como Apache Spark y Apache Kafka para procesar grandes vol√∫menes de datos de usuarios y contenidos en tiempo real.

### **Spotify**

- Utiliza ETL para procesar y analizar datos de uso y preferencias de los usuarios, empleando herramientas como Apache Airflow para la orquestaci√≥n de flujos de datos.

### **Uber**

- Utiliza ETL para manejar datos de viajes, usuarios y conductores, empleando tecnolog√≠as como Apache Hadoop y Apache Kafka.

### **Airbnb**

- Implementa procesos ETL con herramientas como Apache Airflow y Apache Spark para la integraci√≥n y an√°lisis de datos de usuarios y propiedades.

### **LinkedIn**

- Utiliza procesos ETL para gestionar datos de usuarios y conexiones, utilizando tecnolog√≠as como Apache Kafka y Apache Samza.

### **Empresas de Tecnolog√≠a y Consultor√≠a**

- **Accenture, Capgemini, Deloitte**: Utilizan herramientas y procesos ETL para ayudar a sus clientes en la integraci√≥n de datos y la transformaci√≥n digital.
- **SAP**: Ofrece soluciones ETL como SAP Data Services para la integraci√≥n y limpieza de datos empresariales.

### **Instituciones Financieras**

- **JPMorgan Chase, Bank of America, Wells Fargo**: Utilizan procesos ETL para la integraci√≥n de datos financieros, an√°lisis de riesgos y cumplimiento normativo.

### **Sector de Salud**

- **UnitedHealth Group, Anthem, Mayo Clinic**: Implementan ETL para integrar datos de pacientes, historiales m√©dicos y an√°lisis cl√≠nicos.

### **Retail y Comercio Electr√≥nico**

- **Walmart, Target, Alibaba**: Utilizan ETL para integrar y analizar datos de ventas, inventarios y clientes.

Las empresas de diversos sectores implementan procesos ETL para manejar, transformar y analizar grandes vol√∫menes de datos provenientes de m√∫ltiples fuentes. La capacidad de integrar y preparar datos de manera eficiente es fundamental para tomar decisiones informadas y obtener insights valiosos, lo cual es cr√≠tico en la era de los datos y la inteligencia artificial.

## Caso de uso real en Retail y Comercio Electr√≥nico

### GEPP en tus manos [(GRUPO GEPP)](https://gepp.com.mx/)

[![GEPP en tus manos](https://play-lh.googleusercontent.com/QTmd77dyeAdYMfkMXfqOioZq5dkaEnA02raN5CNN0Fzx9Yg0pi1W4t1UWaQcpa8k9us=w526-h296-rw)](https://play.google.com/store/apps/details?id=com.sw1848.pepsilealtad&hl=en_US&pli=1)

GEPP en tus manos es una aplicaci√≥n dise√±ada para mejorar la **comunicaci√≥n con nuestros Clientes Directos GEPP**, te mantendremos actualizado con tu estado en los **programas de fidelizaci√≥n**, conocer√°s el detalle de tus compras y el **cumplimiento de tu cartera GEPP**.
Tambi√©n compartiremos **promociones**, noticias e informaci√≥n interesante sobre sus marcas PEPSI, 7UP, Gatorade, e pura y todas las marcas que conforman la Familia GEPP.

## ¬øCual es el reto a enfrentar en el desarrollo del ETL del core de la APP Gepp en tus manos?

![Caso de uso](/img/etl_use_case.png)

## Soluciones tecnicas implementadas

### **Paralelismo en la lectura**

El paralelismo en la lectura se refiere a la capacidad de realizar m√∫ltiples operaciones de lectura de datos de manera simult√°nea, en lugar de secuencialmente. Esto es especialmente √∫til en procesos ETL donde grandes vol√∫menes de datos necesitan ser procesados eficientemente.

#### Ventajas del Paralelismo en la Lectura

- **Mayor Velocidad**: Permite reducir significativamente el tiempo necesario para leer grandes vol√∫menes de datos.
- **Eficiencia de Recursos**: Aprovecha al m√°ximo los recursos del sistema, como m√∫ltiples n√∫cleos de CPU y memoria distribuida.
- **Escalabilidad**: Facilita el manejo de grandes vol√∫menes de datos a medida que la cantidad de datos y la demanda de procesamiento crecen.

##### Ejemplo de Implementaci√≥n

```python
# üëæ
# Se crea una instancia de la clase Layouts con los archivos especificados en `files`.
layout = LayoutsBase(files)

# Se define un diccionario `tasks` que mapea nombres de tareas a m√©todos espec√≠ficos de la instancia `layout`.
# Cada clave del diccionario es un nombre descriptivo de la tarea, y el valor es el m√©todo correspondiente
# de la clase `Layouts` que se debe ejecutar para completar esa tarea.
tasks = {
    'layout_clients': layout.layout_clients,
    'layout_clients_cdg': layout.layout_clients_cdg,
    'result_layout_program_sales': layout.layout_clients_sales,
    'result_layout_program_portfolio': layout.layout_clients_portfolio,
    'result_layout_promotions': layout.layout_promotions,
    'result_layout_customer_promotions': layout.layout_customer_promotions,
}

# Se utiliza un gestor de contexto para crear un ThreadPoolExecutor, que permite ejecutar las tareas en paralelo.
with concurrent.futures.ThreadPoolExecutor() as executor:
    # Se crea un diccionario `futures` donde cada clave es el nombre de la tarea y el valor es el futuro
    # que representa la tarea as√≠ncrona. `executor.submit` se utiliza para iniciar la ejecuci√≥n de cada tarea.
    futures = {name: executor.submit(task) for name, task in tasks.items()}
    # Se espera a que todas las tareas se completen y se recopilan los resultados en el diccionario `results`.
    # `future.result()` bloquea hasta que la tarea asociada se completa y devuelve el resultado.
    results = {name: future.result() for name, future in futures.items()}

# Se extraen los resultados de las tareas espec√≠ficas del diccionario `results` y se asignan a variables.
layout_grocers_value = results['layout_clients']
layout_grocers_cdg_value = results['layout_clients_cdg']
layout_program_sales_value = results['result_layout_program_sales']
layout_program_portfolio_value = results['result_layout_program_portfolio']
layout_promotions_value = results['result_layout_promotions']
layout_customer_promotions_value = results['result_layout_customer_promotions']

print('parallelism layout base successfully ‚úÖ')
```

### **Particionado de Datos**

El particionado de vol√∫menes grandes de datos es una t√©cnica crucial en el manejo y procesamiento eficiente de grandes cantidades de datos, especialmente en entornos de Big Data. Consiste en dividir un conjunto de datos grande en partes m√°s peque√±as y manejables, llamadas particiones, que pueden ser procesadas en paralelo para mejorar el rendimiento y la escalabilidad.

#### **Ventajas del Particionado de datos**

- **Mejora del Rendimiento**: Permite la lectura y escritura de datos en paralelo, reduciendo el tiempo total de procesamiento.
- **Escalabilidad**: Facilita el manejo de crecientes vol√∫menes de datos al distribuir la carga de trabajo.
- **Mantenimiento Simplificado**: Hacer operaciones de mantenimiento, como copias de seguridad y restauraci√≥n, es m√°s manejable en particiones m√°s peque√±as.
- **Optimizaci√≥n de Consultas**: Las consultas pueden ser m√°s r√°pidas porque solo se escanean las particiones relevantes.

#### **Desaf√≠os del Particionado**

- **Equilibrio de Particiones**: Asegurarse de que las particiones tengan tama√±os equilibrados para evitar cuellos de botella.
- **Administraci√≥n de Metadatos**: Mantener informaci√≥n precisa sobre las particiones puede ser complejo.
- **Consistencia de Datos**: Garantizar que las particiones sean consistentes y que las operaciones de particionamiento no introduzcan errores.

#### **Herramientas y Tecnolog√≠as Comunes**

- **Apache Hadoop**: HDFS permite particionar datos y distribuirlos entre nodos.
- **Apache Spark**: Ofrece particionamiento nativo en RDDs y DataFrames para procesamiento paralelo.
- **Apache Hive**: Soporta particionamiento de tablas para optimizar consultas.
- **SQL Databases**: Bases de datos como PostgreSQL, MySQL, y Oracle soportan particionamiento de tablas.
- **NoSQL Databases**: Bases de datos como Cassandra y MongoDB utilizan particionamiento para distribuir datos.

El particionado de vol√∫menes grandes de datos es esencial para mantener el rendimiento y la escalabilidad en sistemas que manejan grandes cantidades de informaci√≥n. La elecci√≥n de la estrategia de particionado adecuada depende de la naturaleza de los datos y los patrones de acceso. Implementar particionado de manera efectiva puede resultar en una mejora significativa en la eficiencia de los procesos ETL y en la capacidad de manejar datos a gran escala.

##### Ejemplo de Implementaci√≥n

```python
# Se crea una instancia de DataAccessLayer.
instance = DataAccessLayer()

# Se crea una instancia de BuildDataframe.
builder = BuildDataframe()

# Se procesa el layout 'grocers_to_dispute' utilizando el builder y la instancia creada,
# pasando el dataframe 'layout_grocers_value' y especificando el m√©todo '_grocers_' junto con las columnas deseadas.
grocers_to_dispute = process_layout(builder, instance, layout_grocers_value, '_grocers_', ['column name', 'column name'])

# Define una funci√≥n para calcular el n√∫mero de fragmentos en que se debe dividir un dataframe basado en un tama√±o m√°ximo.
def calculate_fragments(dataframe) -> int:
    max = 10000  # Tama√±o m√°ximo de filas por fragmento.
    size_dataframe = len(dataframe)  # Total de filas en el dataframe.
    num_fragments = size_dataframe // max  # N√∫mero b√°sico de fragmentos.
    if size_dataframe % max != 0:  # Si hay un residuo, se necesita un fragmento adicional.
        num_fragments += 1
    return num_fragments

# Define una funci√≥n para dividir un dataframe en 'count' n√∫mero de partes.
def dataframe_partitions(dataset, count):
    partitions = np.array_split(dataset, count)  # Utiliza numpy para dividir el dataframe.
    return partitions

# Procesa un layout aplicando un tipo de dato (si se especifica), calculando fragmentos, y aplicando una funci√≥n a cada fragmento.
def process_layout(builder, instance, dataframe, method, columns, type_cast=None):
    if type_cast:
        dataframe = dataframe.astype(type_cast)  # Cambia el tipo de dato de las columnas si se especifica.
    num = calculate_fragments(dataframe) if len(dataframe) > 10000 else 2  # Calcula el n√∫mero de fragmentos necesarios.
    partitions = dataframe_partitions(dataframe, num)  # Divide el dataframe en fragmentos.
    builder.__attrib__(partitions, instance, method, columns)  # Asigna atributos al builder.
    return builder.__builder__()  # Construye y devuelve el resultado final.

# Clase para construir un dataframe procesado a partir de fragmentos.
class BuildDataframe:

    # Asigna atributos necesarios para el procesamiento.
    def __attrib__(self, partitions, instance, method, columns):
        self.partitions = partitions  # Fragmentos del dataframe.
        self.is_instance = instance  # Instancia de DataAccessLayer.
        self.is_method = method  # M√©todo a aplicar a cada fragmento.
        self.columns = columns  # Columnas a incluir en el procesamiento.

    # Construye el dataframe procesado.
    def __builder__(self):
        self.method_to_dispute = getattr(self.is_instance, self.is_method)  # Obtiene el m√©todo de la instancia.
        self.processed_dataframes = []  # Lista para almacenar dataframes procesados.
        for i, partition_dataset in enumerate(self.partitions):  # Itera sobre cada fragmento.
            dataset = partition_dataset[self.columns]  # Selecciona las columnas especificadas.
            response = self.method_to_dispute(dataset)  # Aplica el m√©todo.
            self.processed_dataframes.append(response)  # A√±ade el resultado a la lista.
        self.response = reduce(DataFrame.union, self.processed_dataframes)  # Combina todos los dataframes procesados.
        return self.response  # Devuelve el dataframe combinado.
```

### **BCP (Bulk Copy Program)**

BCP (Bulk Copy Program) es una herramienta de Microsoft SQL Server que permite importar y exportar grandes vol√∫menes de datos de manera eficiente. BCP es √∫til para la transferencia masiva de datos entre archivos y bases de datos SQL Server, lo que lo convierte en una herramienta valiosa para procesos ETL (Extracci√≥n, Transformaci√≥n y Carga).

### Caracter√≠sticas Principales de BCP

1. **Importaci√≥n y Exportaci√≥n de Datos**: Permite copiar datos entre una tabla de SQL Server y un archivo de datos en formato de texto (TXT, CSV, etc.).
2. **Soporte para Grandes Vol√∫menes de Datos**: Dise√±ado para manejar grandes cantidades de datos de manera eficiente.
3. **Flexibilidad en el Formato de Datos**: Admite varios formatos de datos y permite especificar formatos personalizados.
4. **Integraci√≥n con Otros Procesos**: Puede ser utilizado en scripts y procesos automatizados para la manipulaci√≥n de datos.

### Comandos B√°sicos de BCP

#### Sintaxis General

```bash
bcp {table | view | "query"} {in | out | queryout | format} data_file 
    [-m max_errors] [-f format_file] [-e err_file] [-F first_row] [-L last_row] 
    [-b batch_size] [-n native_type] [-c character_type] [-w wide_character_type] 
    [-N keep_nulls] [-r row_term] [-t field_term] [-T trusted_connection] 
    [-S server_name[\instance_name]] [-U login_id] [-P password]
```

### Par√°metros Comunes

- `-S server_name[\instance_name]`: Nombre del servidor y la instancia.
- `-U login_id`: ID de inicio de sesi√≥n.
- `-P password`: Contrase√±a del usuario.
- `-T`: Utiliza la autenticaci√≥n de Windows.
- `-c`: Utiliza el tipo de datos de car√°cter.
- `-t field_term`: Especifica el delimitador de campo.
- `-r row_term`: Especifica el delimitador de fila.
- `-b batch_size`: N√∫mero de filas por lote de transacci√≥n.
- `-F first_row`: Primera fila a copiar.
- `-L last_row`: √öltima fila a copiar.
- `-m max_errors`: N√∫mero m√°ximo de errores permitidos.

### Mejores Pr√°cticas (Best Practices) para el Uso de BCP

1. **Utilizar Autenticaci√≥n Segura**: Preferir la autenticaci√≥n de Windows (`-T`) sobre la autenticaci√≥n SQL (`-U` y `-P`), cuando sea posible.
2. **Control de Errores**: Utilizar el par√°metro `-e` para especificar un archivo de errores y `-m` para establecer el n√∫mero m√°ximo de errores permitidos.
3. **Optimizaci√≥n de Lotes**: Ajustar el tama√±o del lote (`-b`) para equilibrar la velocidad de carga y la utilizaci√≥n de recursos.
4. **Formato de Datos**: Usar archivos de formato (`-f`) para manejar datos con formatos complejos o personalizados.
5. **Validaci√≥n de Datos**: Validar y limpiar los datos antes de la importaci√≥n para minimizar errores y problemas de integridad.
6. **Paralelizaci√≥n**: Dividir grandes conjuntos de datos en fragmentos m√°s peque√±os y procesarlos en paralelo para mejorar el rendimiento.

##### Ejemplo de Implementaci√≥n

```python
# Define una funci√≥n para ejecutar un comando bcp para importar datos desde un archivo CSV a una tabla de SQL Server.
def run_bcp_command(path_csv, table_name):
    # Construye el comando bcp utilizando variables para la tabla, el archivo CSV, y las credenciales de la base de datos.
    bcp_command = f"/opt/mssql-tools/bin/bcp {table_name} in {path_csv} -c -t, -S {DATABASE_HOST} -d {DATABASE_NAME} -U {DATABASE_USER} -P {DATABASE_PASSWORD} > ./logs/bcp.log 2>&1"
    try:
        # Ejecuta el comando bcp utilizando subprocess.run. `shell=True` permite ejecutar el comando como si estuviera en la shell.
        # `check=True` hace que se lance una excepci√≥n si el comando falla.
        subprocess.run(bcp_command, shell=True, check=True)
    except subprocess.CalledProcessError as e:
        # Captura cualquier error que ocurra durante la ejecuci√≥n del comando bcp y lo imprime.
        print(f"Error executing bcp command: {e}")

```

## **Arquitectura y DevOps**

### Pasado execution time 8.37 hrs

![Infra](/img/architecture_microsoft_server.png)

### Presente execution time 32 min

![Infra](/img/architecture_azure.png)
<!-- 
## **Consejo de la comunidad MongoDB**

Microsoft Data Platform MVP, MongoDB Certified Developer
[Leandro Domingues](https://www.mongodb.com/community/advocacy/leandro-domingues/)

![MongoDB Conference](/img/photo_1.jpg)
![Consejo](/img/photo_2.jpg)

## **"Documentos y schemas flexibles son el nuevo petroleo piensen sobre eso."**

## **Demo**

Explicaci√≥n del c√≥digo de este repositorio -->

### **Consideraciones Finales**

El proceso ETL es una columna vertebral imprescindible en cualquier proyecto de Inteligencia Artificial. Asegura que los datos necesarios est√©n disponibles, sean de alta calidad y est√©n en el formato correcto. Sin un proceso ETL robusto, la creaci√≥n, entrenamiento y despliegue de modelos de IA ser√≠a significativamente m√°s complejo y menos eficiente. Por lo tanto, dominar las t√©cnicas y herramientas de ETL no solo es beneficioso, sino esencial para cualquier profesional que trabaje en el campo de la IA y el an√°lisis de datos.